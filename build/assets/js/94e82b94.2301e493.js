"use strict";(globalThis.webpackChunktemp_site=globalThis.webpackChunktemp_site||[]).push([[288],{6145:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"week-13-conversational/whisper-speech-to-text","title":"Week 13: Conversational AI - Speech-to-Text with Whisper","description":"This guide explains how to integrate a powerful speech-to-text (STT) model, OpenAI\'s Whisper, into a ROS 2 application. This will allow our robot to understand voice commands from a user.","source":"@site/docs/week-13-conversational/whisper-speech-to-text.md","sourceDirName":"week-13-conversational","slug":"/week-13-conversational/whisper-speech-to-text","permalink":"/docs/week-13-conversational/whisper-speech-to-text","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/week-13-conversational/whisper-speech-to-text.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Week 13: Conversational AI - Multimodal Interfaces for Human-Robot Interaction","permalink":"/docs/week-13-conversational/multimodal-interfaces"},"next":{"title":"Why Robotics? A Deep Dive into the Motivation and Impact","permalink":"/docs/why_robotics"}}');var t=s(4848),o=s(8453);const r={},a="Week 13: Conversational AI - Speech-to-Text with Whisper",l={},c=[{value:"1. What is OpenAI&#39;s Whisper?",id:"1-what-is-openais-whisper",level:2},{value:"Key Features:",id:"key-features",level:3},{value:"2. System Design",id:"2-system-design",level:2},{value:"3. Installation and Setup",id:"3-installation-and-setup",level:2},{value:"Step 3.1: Install Whisper",id:"step-31-install-whisper",level:3},{value:"Step 3.2: Setting up an Audio Source in ROS 2",id:"step-32-setting-up-an-audio-source-in-ros-2",level:3},{value:"4. Creating the Whisper ROS 2 Node",id:"4-creating-the-whisper-ros-2-node",level:2},{value:"How the Node Works:",id:"how-the-node-works",level:3},{value:"5. Launching and Testing",id:"5-launching-and-testing",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"week-13-conversational-ai---speech-to-text-with-whisper",children:"Week 13: Conversational AI - Speech-to-Text with Whisper"})}),"\n",(0,t.jsx)(n.p,{children:"This guide explains how to integrate a powerful speech-to-text (STT) model, OpenAI's Whisper, into a ROS 2 application. This will allow our robot to understand voice commands from a user."}),"\n",(0,t.jsx)(n.h2,{id:"1-what-is-openais-whisper",children:"1. What is OpenAI's Whisper?"}),"\n",(0,t.jsx)(n.p,{children:"Whisper is a state-of-the-art automatic speech recognition (ASR) model developed by OpenAI. It was trained on a massive and diverse dataset of audio from the internet, making it incredibly robust against accents, background noise, and technical language."}),"\n",(0,t.jsx)(n.h3,{id:"key-features",children:"Key Features:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"High Accuracy:"})," It achieves human-level performance on many benchmarks."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multilingual:"})," It can transcribe audio in dozens of languages."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robustness:"})," Performs well even with significant background noise."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Open Source:"})," The model and its code are open source, allowing you to run it locally."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"For our Butler Bot project, Whisper is an excellent choice for converting the user's spoken command (e.g., \"Bot, bring me the red can\") into text that our robot's AI can process."}),"\n",(0,t.jsx)(n.h2,{id:"2-system-design",children:"2. System Design"}),"\n",(0,t.jsx)(n.p,{children:"We will create a ROS 2 node that:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Listens for raw audio data from a microphone."}),"\n",(0,t.jsx)(n.li,{children:"When it detects speech, it captures an audio clip."}),"\n",(0,t.jsx)(n.li,{children:"Sends this audio clip to the Whisper model for transcription."}),"\n",(0,t.jsx)(n.li,{children:"Publishes the resulting text to a ROS 2 topic."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"3-installation-and-setup",children:"3. Installation and Setup"}),"\n",(0,t.jsx)(n.h3,{id:"step-31-install-whisper",children:"Step 3.1: Install Whisper"}),"\n",(0,t.jsxs)(n.p,{children:["First, you need to install the ",(0,t.jsx)(n.code,{children:"openai-whisper"})," Python package. You will also need ",(0,t.jsx)(n.code,{children:"ffmpeg"})," for audio processing."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Install ffmpeg system-wide\nsudo apt update\nsudo apt install ffmpeg\n\n# Install the whisper package via pip\npip install openai-whisper\n"})}),"\n",(0,t.jsx)(n.p,{children:"Depending on your hardware, you may also need to install PyTorch with CUDA support to get the best performance."}),"\n",(0,t.jsx)(n.h3,{id:"step-32-setting-up-an-audio-source-in-ros-2",children:"Step 3.2: Setting up an Audio Source in ROS 2"}),"\n",(0,t.jsxs)(n.p,{children:["You need a ROS 2 node that can publish microphone data. A common way to do this is using the ",(0,t.jsx)(n.code,{children:"audio_common"})," package."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Install the audio_common package for your ROS distro\nsudo apt install ros-humble-audio-common\n"})}),"\n",(0,t.jsxs)(n.p,{children:["You can then run the ",(0,t.jsx)(n.code,{children:"audio_capture"})," node to publish microphone data to the ",(0,t.jsx)(n.code,{children:"/audio"})," topic."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Launch the microphone capture node\nros2 run audio_capture audio_capture_node\n"})}),"\n",(0,t.jsxs)(n.p,{children:["This will publish messages of type ",(0,t.jsx)(n.code,{children:"audio_common_msgs/AudioData"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"4-creating-the-whisper-ros-2-node",children:"4. Creating the Whisper ROS 2 Node"}),"\n",(0,t.jsx)(n.p,{children:"Now, let's create the Python node that will perform the transcription."}),"\n",(0,t.jsxs)(n.p,{children:["Create a new file ",(0,t.jsx)(n.code,{children:"whisper_node.py"})," in your ROS 2 package."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\n\nimport whisper\nimport numpy as np\nimport torch\n\n# Check if CUDA is available\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass WhisperNode(Node):\n\n    def __init__(self):\n        super().__init__('whisper_node')\n        \n        # Load the Whisper model\n        self.get_logger().info('Loading Whisper model...')\n        self.model = whisper.load_model(\"base.en\", device=DEVICE) # Use \"base.en\" for English\n        self.get_logger().info('Whisper model loaded.')\n\n        # Create subscriber to the audio topic\n        self.audio_subscriber = self.create_subscription(\n            AudioData,\n            '/audio',\n            self.audio_callback,\n            10)\n        \n        # Create publisher for the transcribed text\n        self.text_publisher = self.create_publisher(String, '/transcribed_text', 10)\n\n        # Buffer to store audio data\n        self.audio_buffer = []\n        self.get_logger().info('Whisper node has started.')\n\n    def audio_callback(self, msg):\n        # The audio data is a list of bytes. We need to convert it to a NumPy array.\n        # Assuming 16-bit signed integer format, which is common.\n        audio_np = np.frombuffer(msg.data, dtype=np.int16)\n        \n        # Convert to float32, which is what Whisper expects\n        audio_float32 = audio_np.astype(np.float32) / 32768.0\n        \n        # For simplicity, we process every incoming message.\n        # A more advanced implementation would use Voice Activity Detection (VAD)\n        # to buffer audio only when someone is speaking.\n        \n        self.get_logger().info('Transcribing audio...')\n        try:\n            # Transcribe\n            result = self.model.transcribe(audio_float32, fp16=torch.cuda.is_available())\n            transcribed_text = result['text']\n            \n            if transcribed_text:\n                self.get_logger().info(f\"Transcribed: {transcribed_text}\")\n                \n                # Publish the transcribed text\n                text_msg = String()\n                text_msg.data = transcribed_text\n                self.text_publisher.publish(text_msg)\n                \n        except Exception as e:\n            self.get_logger().error(f\"Error during transcription: {e}\")\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    whisper_node = WhisperNode()\n    rclpy.spin(whisper_node)\n    whisper_node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n\n"})}),"\n",(0,t.jsx)(n.h3,{id:"how-the-node-works",children:"How the Node Works:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Initialization:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["It loads a pre-trained Whisper model. We use ",(0,t.jsx)(n.code,{children:"base.en"}),", which is a small, fast, English-only model. For higher accuracy, you could use ",(0,t.jsx)(n.code,{children:"medium.en"})," or ",(0,t.jsx)(n.code,{children:"large"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["It creates a subscriber to the ",(0,t.jsx)(n.code,{children:"/audio"})," topic and a publisher for the ",(0,t.jsx)(n.code,{children:"/transcribed_text"})," topic."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:[(0,t.jsx)(n.code,{children:"audio_callback"}),":"]})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["This function is called every time a new ",(0,t.jsx)(n.code,{children:"AudioData"})," message arrives."]}),"\n",(0,t.jsx)(n.li,{children:"It converts the raw audio bytes into a NumPy array of floating-point numbers, which is the format Whisper requires."}),"\n",(0,t.jsxs)(n.li,{children:["It calls ",(0,t.jsx)(n.code,{children:"self.model.transcribe()"})," on the audio data."]}),"\n",(0,t.jsxs)(n.li,{children:["If the transcription is successful and returns text, it publishes that text as a ",(0,t.jsx)(n.code,{children:"std_msgs/String"})," on the ",(0,t.jsx)(n.code,{children:"/transcribed_text"})," topic."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"5-launching-and-testing",children:"5. Launching and Testing"}),"\n",(0,t.jsx)(n.p,{children:"To test the system, you need to run three components:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"audio_capture"})," node."]}),"\n",(0,t.jsxs)(n.li,{children:["Your ",(0,t.jsx)(n.code,{children:"whisper_node"}),"."]}),"\n",(0,t.jsx)(n.li,{children:"A ROS 2 topic echo to listen for the output."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Terminal 1: Audio Capture"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"source /opt/ros/humble/setup.bash\nros2 run audio_capture audio_capture_node\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Terminal 2: Whisper Node"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Make sure your ROS 2 package is built and sourced\nsource install/setup.bash\nros2 run your_package_name whisper_node\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Terminal 3: Listen for Text"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /transcribed_text\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Now, speak into your microphone. You should see the ",(0,t.jsx)(n.code,{children:"whisper_node"})," log that it is transcribing, and the final text should appear in Terminal 3. This text can now be used as an input to your robot's behavior tree or main logic controller."]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>a});var i=s(6540);const t={},o=i.createContext(t);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);